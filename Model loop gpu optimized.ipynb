{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "057cae87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from numpy.random import seed # to set seed\n",
    "from tensorflow.random import set_seed # to set seed\n",
    "import pandas as pd\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bfe0beeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply seed\n",
    "seed_value = 1338\n",
    "set_seed(seed_value)\n",
    "seed(seed_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69e26a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TF_XLA_FLAGS'] = '--tf_xla_enable_xla_devices'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c07dc56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.is_built_with_cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44ca9705",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26b04dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#conda activate -n gpu2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1843032",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2076 images belonging to 2 classes.\n",
      "Found 258 images belonging to 2 classes.\n",
      "Found 262 images belonging to 2 classes.\n",
      "Found 2076 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Kopieret fra model_2.py 4/11 ~~11.30\n",
    "#%% ImageDataGenerator\n",
    "\n",
    "image_size = [200, 200]\n",
    "batch = 32\n",
    "\n",
    "train_datagen = ImageDataGenerator(rotation_range=40,\n",
    "                                   rescale=1./255,\n",
    "                                   shear_range=0.1,\n",
    "                                   zoom_range=0.2,\n",
    "                                   horizontal_flip= True,\n",
    "                                   vertical_flip=True,\n",
    "                                   width_shift_range=0.1,\n",
    "                                   height_shift_range=0.1)\n",
    "\n",
    "train_gen = train_datagen.flow_from_directory(\n",
    "    'train',\n",
    "    target_size=image_size,\n",
    "    batch_size=batch,\n",
    "    class_mode='binary')\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "val_gen = test_datagen.flow_from_directory(\n",
    "        'val',\n",
    "        target_size=image_size,\n",
    "        batch_size=batch,\n",
    "        class_mode='binary')\n",
    "\n",
    "test_gen = test_datagen.flow_from_directory(\n",
    "    'test',\n",
    "    target_size=image_size,\n",
    "    batch_size=batch,\n",
    "    class_mode='binary')\n",
    "\n",
    "#%% Visualizing ImageDataGenerator augmentation\n",
    "train_datagen_viz = ImageDataGenerator(rotation_range=40,\n",
    "                                   shear_range=0.1,\n",
    "                                   zoom_range=0.2,\n",
    "                                   horizontal_flip= True,\n",
    "                                   vertical_flip=True,\n",
    "                                   width_shift_range=0.1,\n",
    "                                   height_shift_range=0.1)\n",
    "\n",
    "train_gen_viz = train_datagen_viz.flow_from_directory(\n",
    "    'train',\n",
    "    target_size=image_size,\n",
    "    color_mode='rgb',\n",
    "    batch_size=1,\n",
    "    class_mode='binary',\n",
    "    seed=1337)\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=4, figsize=(15,15))\n",
    "\n",
    "for i in range(4):\n",
    "    image = next(train_gen_viz)[0].astype('uint8')\n",
    "    \n",
    "    image = np.squeeze(image)\n",
    "    \n",
    "    ax[i].imshow(image)\n",
    "    ax[i].axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d00bc2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Units\n",
    "con_layer_1 = [16]\n",
    "con_layer_2 = [32]\n",
    "con_layer_3 = [32]\n",
    "act_funcs = ['relu', 'selu', 'elu', 'tanh', 'sigmoid']\n",
    "hidden_layer_1 = [512]\n",
    "dropout_sizes = [0.2]\n",
    "optimizers = [\"adam\", \"sgd\"]\n",
    "epoch_n = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ccae95c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 250 parameter combinations and 10 epochs\n"
     ]
    }
   ],
   "source": [
    "# Number of models to be created:\n",
    "model_count = len(con_layer_1) * len(con_layer_2) * len(con_layer_3) * len(act_funcs) * len(act_funcs) * len(act_funcs) * len(act_funcs) * len(dropout_sizes) * len(optimizers) * len(hidden_layer_1)\n",
    "\n",
    "# Result array\n",
    "results = pd.DataFrame(columns = ['loss',\n",
    "                                  'val_loss',\n",
    "                                  'accuracy',\n",
    "                                  'val_accuracy',\n",
    "                                  'con_layer_1',\n",
    "                                  'con_layer_1_activation',\n",
    "                                  'con_layer_2',\n",
    "                                  'con_layer_2_activation',\n",
    "                                  'con_layer_3',\n",
    "                                  'con_layer_3_activation',\n",
    "                                  'hidden_layer_1',\n",
    "                                  'hidden_layer_1_activation',\n",
    "                                  'dropout_sizes',\n",
    "                                  'optimizers',\n",
    "                                  'epoch',\n",
    "                                  'epoch_n'])\n",
    "print(f\"There are {model_count} parameter combinations and {epoch_n} epochs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "06bba3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load results\n",
    "results = pd.read_csv('loop_results.csv')\n",
    "#results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d3447c43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped model 1 out of 250.\n",
      "Skipped model 2 out of 250.\n",
      "Skipped model 3 out of 250.\n",
      "Skipped model 4 out of 250.\n",
      "Skipped model 5 out of 250.\n",
      "Skipped model 6 out of 250.\n",
      "Skipped model 7 out of 250.\n",
      "Skipped model 8 out of 250.\n",
      "Skipped model 9 out of 250.\n",
      "Skipped model 10 out of 250.\n",
      "Skipped model 11 out of 250.\n",
      "Skipped model 12 out of 250.\n",
      "Skipped model 13 out of 250.\n",
      "Skipped model 14 out of 250.\n",
      "Skipped model 15 out of 250.\n",
      "Skipped model 16 out of 250.\n",
      "Skipped model 17 out of 250.\n",
      "Skipped model 18 out of 250.\n",
      "Skipped model 19 out of 250.\n",
      "Skipped model 20 out of 250.\n",
      "Skipped model 21 out of 250.\n",
      "Skipped model 22 out of 250.\n",
      "Skipped model 23 out of 250.\n",
      "Skipped model 24 out of 250.\n",
      "Skipped model 25 out of 250.\n",
      "Skipped model 26 out of 250.\n",
      "Skipped model 27 out of 250.\n",
      "Skipped model 28 out of 250.\n",
      "Skipped model 29 out of 250.\n",
      "Skipped model 30 out of 250.\n",
      "Skipped model 31 out of 250.\n",
      "Skipped model 32 out of 250.\n",
      "Skipped model 33 out of 250.\n",
      "Skipped model 34 out of 250.\n",
      "Skipped model 35 out of 250.\n",
      "Skipped model 36 out of 250.\n",
      "Skipped model 37 out of 250.\n",
      "Skipped model 38 out of 250.\n",
      "Skipped model 39 out of 250.\n",
      "Skipped model 40 out of 250.\n",
      "Skipped model 41 out of 250.\n",
      "Skipped model 42 out of 250.\n",
      "Skipped model 43 out of 250.\n",
      "Skipped model 44 out of 250.\n",
      "Skipped model 45 out of 250.\n",
      "Skipped model 46 out of 250.\n",
      "Skipped model 47 out of 250.\n",
      "Skipped model 48 out of 250.\n",
      "Skipped model 49 out of 250.\n",
      "Skipped model 50 out of 250.\n",
      "Skipped model 51 out of 250.\n",
      "Skipped model 52 out of 250.\n",
      "Skipped model 53 out of 250.\n",
      "Skipped model 54 out of 250.\n",
      "Skipped model 55 out of 250.\n",
      "Skipped model 56 out of 250.\n",
      "Skipped model 57 out of 250.\n",
      "Skipped model 58 out of 250.\n",
      "Skipped model 59 out of 250.\n",
      "Skipped model 60 out of 250.\n",
      "Skipped model 61 out of 250.\n",
      "Skipped model 62 out of 250.\n",
      "Skipped model 63 out of 250.\n",
      "Skipped model 64 out of 250.\n",
      "Skipped model 65 out of 250.\n",
      "Skipped model 66 out of 250.\n",
      "Skipped model 67 out of 250.\n",
      "Skipped model 68 out of 250.\n",
      "Skipped model 69 out of 250.\n",
      "Skipped model 70 out of 250.\n",
      "Skipped model 71 out of 250.\n",
      "Skipped model 72 out of 250.\n",
      "Skipped model 73 out of 250.\n",
      "Skipped model 74 out of 250.\n",
      "Skipped model 75 out of 250.\n",
      "Skipped model 76 out of 250.\n",
      "Skipped model 77 out of 250.\n",
      "Skipped model 78 out of 250.\n",
      "Skipped model 79 out of 250.\n",
      "Skipped model 80 out of 250.\n",
      "Skipped model 81 out of 250.\n",
      "Skipped model 82 out of 250.\n",
      "Skipped model 83 out of 250.\n",
      "Skipped model 84 out of 250.\n",
      "Skipped model 85 out of 250.\n",
      "Skipped model 86 out of 250.\n",
      "Skipped model 87 out of 250.\n",
      "Skipped model 88 out of 250.\n",
      "Skipped model 89 out of 250.\n",
      "Training model 90 out of 250.\n",
      "WARNING:tensorflow:From C:\\Users\\Max\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
      "Instructions for updating:\n",
      "use `tf.profiler.experimental.stop` instead.\n",
      "Training model 91 out of 250.\n",
      "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.6561s vs `on_train_batch_end` time: 1.0154s). Check your callbacks.\n",
      "Training model 92 out of 250.\n",
      "Training model 93 out of 250.\n",
      "Training model 94 out of 250.\n",
      "Training model 95 out of 250.\n",
      "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.6561s vs `on_train_batch_end` time: 0.9842s). Check your callbacks.\n",
      "Training model 96 out of 250.\n",
      "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.5467s vs `on_train_batch_end` time: 0.9373s). Check your callbacks.\n",
      "Training model 97 out of 250.\n",
      "Training model 98 out of 250.\n",
      "Training model 99 out of 250.\n",
      "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.6719s vs `on_train_batch_end` time: 1.0310s). Check your callbacks.\n",
      "Training model 100 out of 250.\n",
      "Training model 101 out of 250.\n",
      "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.6717s vs `on_train_batch_end` time: 1.0466s). Check your callbacks.\n",
      "Training model 102 out of 250.\n",
      "Training model 103 out of 250.\n",
      "Training model 104 out of 250.\n",
      "Training model 105 out of 250.\n",
      "Training model 106 out of 250.\n",
      "Training model 107 out of 250.\n",
      "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.6717s vs `on_train_batch_end` time: 1.0154s). Check your callbacks.\n",
      "Training model 108 out of 250.\n",
      "Training model 109 out of 250.\n",
      "Training model 110 out of 250.\n",
      "Training model 111 out of 250.\n",
      "Training model 112 out of 250.\n",
      "Training model 113 out of 250.\n",
      "Training model 114 out of 250.\n",
      "Training model 115 out of 250.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-902bed33454e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m                                         \u001b[1;31m# Fitting model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 90\u001b[1;33m                                         history = model.fit(\n\u001b[0m\u001b[0;32m     91\u001b[0m                                             \u001b[0mtrain_gen\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m                                             \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mSTEP_SIZE_TRAIN\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[0;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1098\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1099\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    778\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    805\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    806\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 807\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    808\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    809\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2829\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2830\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2831\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1841\u001b[0m       \u001b[0;31m`\u001b[0m\u001b[0margs\u001b[0m\u001b[0;31m`\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1842\u001b[0m     \"\"\"\n\u001b[1;32m-> 1843\u001b[1;33m     return self._call_flat(\n\u001b[0m\u001b[0;32m   1844\u001b[0m         [t for t in nest.flatten((args, kwargs), expand_composites=True)\n\u001b[0;32m   1845\u001b[0m          if isinstance(t, (ops.Tensor,\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1921\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1922\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1923\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1924\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    543\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    546\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Loop tracking\n",
    "count = 0 # To count loops\n",
    "# Model loop\n",
    "for con1 in con_layer_1:\n",
    "    for con1_act in act_funcs:\n",
    "        for con2 in con_layer_2:\n",
    "            if(con1*3<con2):\n",
    "                continue\n",
    "            for con2_act in act_funcs:\n",
    "                for con3 in con_layer_3:\n",
    "                    if(con2*3<con3):\n",
    "                        continue\n",
    "                    for con3_act in act_funcs:\n",
    "                        for hidden1 in hidden_layer_1:\n",
    "                            for hidden1_act in act_funcs:\n",
    "                                for dropout_size in dropout_sizes:\n",
    "                                    for optimizer in optimizers:\n",
    "                                        # Count and print progress\n",
    "                                        count = count+1\n",
    "                                        \n",
    "                                        \n",
    "                                        # Check if combination has been used\n",
    "                                        if(len(results.loc[(results['con_layer_1'] == con1) &\n",
    "                                                           (results['con_layer_1_activation'] <= con1_act) &\n",
    "                                                           (results['con_layer_2'] == con2) &\n",
    "                                                           (results['con_layer_2_activation'] == con2_act) &\n",
    "                                                           (results['con_layer_3'] == con3) &\n",
    "                                                           (results['con_layer_3_activation'] == con3_act) &\n",
    "                                                           (results['hidden_layer_1'] == hidden1) &\n",
    "                                                           (results['hidden_layer_1_activation'] == hidden1_act) &\n",
    "                                                           (results['dropout_sizes'] == dropout_size) &\n",
    "                                                           (results['optimizers'] == optimizer) &\n",
    "                                                           (results['epoch_n'] <= epoch_n)])):\n",
    "                                            print(f\"Skipped model {count} out of {model_count}.\")\n",
    "                                            continue\n",
    "                                        else:\n",
    "                                            print(f'Training model {count} out of {model_count}.')\n",
    "                                        # Model baseret på model.py kopieret 04/11 ~11.30\n",
    "                                        model = keras.Sequential([\n",
    "                                            # First convolution\n",
    "                                            layers.Conv2D(con1, (3,3), activation=con1_act, input_shape=(200, 200, 3)),\n",
    "                                            layers.MaxPooling2D(2,2), # halving the image size \n",
    "\n",
    "                                            # Second convolution\n",
    "                                            layers.Conv2D(con2, (3,3), activation=con2_act),\n",
    "                                            layers.MaxPooling2D(2,2),\n",
    "\n",
    "                                            # Third convolution\n",
    "                                            layers.Conv2D(con3, (3,3), activation=con3_act),\n",
    "                                            layers.MaxPooling2D(2,2),\n",
    "\n",
    "                                            # Fourth convolution - hidden until after some finetuning\n",
    "                                            #layers.Conv2D(64, (3,3), activation='relu'),\n",
    "                                            #layers.MaxPooling2D(2,2),\n",
    "\n",
    "                                            # Fifth convolution - hidden until after some finetuning\n",
    "                                            #layers.Conv2D(64, (3,3), activation='relu'),\n",
    "                                            #layers.MaxPooling2D(2,2),\n",
    "\n",
    "                                            # Flatten results to feed into a Deep Nerual Net\n",
    "                                            layers.Flatten(),\n",
    "\n",
    "                                            # 512 neuron hidden layer\n",
    "                                            layers.Dense(hidden1, activation=hidden1_act),\n",
    "                                            \n",
    "                                            # Dropout NEW!!!!!!\n",
    "                                            layers.Dropout(dropout_size),\n",
    "\n",
    "                                            # Binary output layer\n",
    "                                            layers.Dense(1, activation='sigmoid')\n",
    "                                            ])\n",
    "\n",
    "                                        #model.summary() # model summary\n",
    "\n",
    "                                        model.compile(\n",
    "                                            loss='binary_crossentropy',\n",
    "                                            optimizer=optimizer,\n",
    "                                            metrics=['accuracy']) # compiling model\n",
    "                                        \n",
    "                                        # Model fitting fra model_2.py kopieret 4/11 ~11.30\n",
    "                                        # Callbacks for tensorboard \n",
    "                                        tensorboard_callback = keras.callbacks.TensorBoard(log_dir=\"./logs\") # tensorboard --logdir ./logs\n",
    "\n",
    "                                        # Step sizes for train, validation and testing\n",
    "                                        STEP_SIZE_TRAIN=train_gen.n//train_gen.batch_size\n",
    "                                        STEP_SIZE_VAL=val_gen.n//val_gen.batch_size\n",
    "                                        STEP_SIZE_TEST=test_gen.n//test_gen.batch_size\n",
    "\n",
    "                                        # Fitting model\n",
    "                                        history = model.fit(\n",
    "                                            train_gen,\n",
    "                                            steps_per_epoch=STEP_SIZE_TRAIN,\n",
    "                                            epochs=epoch_n, # for at spare lidt tid\n",
    "                                            validation_data=val_gen,\n",
    "                                            validation_steps=STEP_SIZE_VAL,\n",
    "                                            callbacks=[tensorboard_callback],\n",
    "                                            verbose = False)\n",
    "                                        \n",
    "                                        # Get results (from history)\n",
    "                                        history = history.history\n",
    "                                        \n",
    "                                        # Append each epoch\n",
    "                                        for h in range(len(history['loss'])):\n",
    "                                            row = {'loss':history['loss'][h],\n",
    "                                                   'val_loss':history['val_loss'][h],\n",
    "                                                   'accuracy':history['accuracy'][h],\n",
    "                                                   'val_accuracy': history['val_accuracy'][h],\n",
    "                                                   'con_layer_1': con1,\n",
    "                                                   'con_layer_1_activation': con1_act,\n",
    "                                                   'con_layer_2': con2,\n",
    "                                                   'con_layer_2_activation': con2_act,\n",
    "                                                   'con_layer_3': con3,\n",
    "                                                   'con_layer_3_activation': con3_act,\n",
    "                                                   'hidden_layer_1': hidden1,\n",
    "                                                   'hidden_layer_1_activation': hidden1_act,\n",
    "                                                   'dropout_sizes': dropout_size,\n",
    "                                                   'optimizers': optimizer,\n",
    "                                                   'epoch': (h+1),\n",
    "                                                   'epoch_n': epoch_n}\n",
    "                                            results = results.append(row, ignore_index=True)\n",
    "\n",
    "print(\"All done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e4dd128a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "results.to_csv('loop_results.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
